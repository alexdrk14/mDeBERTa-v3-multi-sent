Model trained on: sst2, amazon_rev_sample_s1, stanfordnlp_sample_s1

Fine-tuned 1 epoch of the mDeBerta:
sst2
Model - F1: 0.9034, Acc: 0.9037
-GPT4 - F1: 0.0000, Acc: 0.9310
-------------------------------
sent-eng
Model - F1: 0.3452, Acc: 0.3670
-GPT4 - F1: 0.4611, Acc: 0.5870
-------------------------------
sent-twi
Model - F1: 0.2886, Acc: 0.4236
-GPT4 - F1: 0.5049, Acc: 0.5385
-------------------------------
absc-laptop
Model - F1: 0.4848, Acc: 0.6513
-GPT4 - F1: 0.6679, Acc: 0.7642
-------------------------------
absc-rest
Model - F1: 0.5245, Acc: 0.7531
-GPT4 - F1: 0.7057, Acc: 0.8385
-------------------------------
Fine-tuned 2 epoch of the mDeBerta:
sst2
Model - F1: 0.9105, Acc: 0.9106
-GPT4 - F1: 0.0000, Acc: 0.9310
-------------------------------
sent-eng
Model - F1: 0.3507, Acc: 0.3720
-GPT4 - F1: 0.4611, Acc: 0.5870
-------------------------------
sent-twi
Model - F1: 0.2831, Acc: 0.4162
-GPT4 - F1: 0.5049, Acc: 0.5385
-------------------------------
absc-laptop
Model - F1: 0.4648, Acc: 0.6237
-GPT4 - F1: 0.6679, Acc: 0.7642
-------------------------------
absc-rest
Model - F1: 0.5133, Acc: 0.7407
-GPT4 - F1: 0.7057, Acc: 0.8385
-------------------------------